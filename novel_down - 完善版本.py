# -*- coding: utf-8 -*-
from DrissionPage import ChromiumPage
import os
import time
import re
import random

class NovelDownloader:
    def __init__(self, target_url, base_save_path):
        self.target_url = target_url
        self.base_save_path = base_save_path
        print("[ç³»ç»Ÿ] æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
        self.page = ChromiumPage()
        self.page.set.timeouts(15)

    def validate_filename(self, filename):
        return re.sub(r'[\\/:*?"<>|]', '_', filename).strip()

    def clean_title(self, text):
        match = re.search(r'(ç¬¬[0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡ä¸¤]+ç« .+)', text)
        if match:
            return match.group(1).strip()
        return text.strip()

    def find_catalog_container(self):
        print("[ç³»ç»Ÿ] æ­£åœ¨æ™ºèƒ½è¯†åˆ«ç›®å½•åŒºåŸŸ...")
        candidates = self.page.eles('tag:div') + self.page.eles('tag:dl') + self.page.eles('tag:ul')
        best_container = None
        max_chapter_count = 0
        
        for ele in candidates:
            try:
                if len(ele.text) < 200: continue
                links = ele.eles('tag:a')
                count = 0
                for link in links:
                    if "ç¬¬" in link.text or "ç« " in link.text:
                        count += 1
                if count > max_chapter_count:
                    max_chapter_count = count
                    best_container = ele
            except:
                continue
        return best_container

    def parse_catalog(self):
        print(f"[1/2] æ­£åœ¨è®¿é—®ç›®å½•é¡µ: {self.target_url}")
        self.page.get(self.target_url)
        
        if "Just a moment" in self.page.title or "éªŒè¯" in self.page.title:
            print("\n[æ³¨æ„] è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨é€šè¿‡ Cloudflare éªŒè¯ï¼ç­‰å¾… 15 ç§’...\n")
            time.sleep(15)

        try:
            h1 = self.page.ele('tag:h1')
            book_title = h1.text if h1 else "æœªçŸ¥å°è¯´"
        except:
            book_title = "æœªçŸ¥å°è¯´"
        book_title = self.validate_filename(book_title)
        print(f"[ç³»ç»Ÿ] è¯†åˆ«ä¹¦å: {book_title}")

        container = self.find_catalog_container()
        if not container:
            container = self.page

        print("[ç³»ç»Ÿ] æ­£åœ¨æŠ“å–ç« èŠ‚åˆ—è¡¨...")
        chapters = []
        seen_urls = set()
        
        all_links = container.eles('tag:a')
        special_keywords = ['åº', 'å¼•å­', 'æ¥”å­', 'å°¾å£°', 'åè®°', 'æ„Ÿè¨€', 'ç•ªå¤–', 'å®Œæœ¬']

        for link in all_links:
            try:
                raw_text = link.text
                url = link.link
                if not url or url in seen_urls: continue
                
                clean_name = self.clean_title(raw_text)
                is_standard = "ç¬¬" in clean_name and "ç« " in clean_name
                is_special = any(k in clean_name for k in special_keywords)
                
                if not is_standard and not is_special:
                    continue
                if clean_name in ["æœ€æ–°ç« èŠ‚", "å…¨éƒ¨ç« èŠ‚", "åˆ†å·é˜…è¯»", "åŠ å…¥ä¹¦æ¶"]:
                    continue

                chapters.append({'name': clean_name, 'url': url})
                seen_urls.add(url)
            except:
                continue

        count = len(chapters)
        if count == 0:
            print("[é”™è¯¯] æœªæŠ“å–åˆ°ç« èŠ‚é“¾æ¥ã€‚")
            return None, None

        print(f"[ç³»ç»Ÿ] æˆåŠŸè§£æåˆ° {count} ä¸ªç« èŠ‚ã€‚")
        return book_title, chapters

    def parse_content(self, chapter_url):
        self.page.get(chapter_url)
        content_ele = None
        selectors = ['#contentbox', '.contentbox', '#content', '.content', '.read-content']
        
        for selector in selectors:
            try:
                if self.page.ele(selector, timeout=5):
                    ele = self.page.ele(selector)
                    if len(ele.text) > 50:
                        content_ele = ele
                        break
            except:
                continue
        
        if not content_ele:
            try:
                divs = self.page.eles('tag:div')
                max_len = 0
                for div in divs:
                    if len(div.eles('tag:a')) > 5: continue
                    txt_len = len(div.text)
                    if txt_len > max_len:
                        max_len = txt_len
                        content_ele = div
            except:
                pass

        if not content_ele:
            return None

        lines = []
        p_tags = content_ele.eles('tag:p')
        if p_tags:
            for p in p_tags:
                text = p.text.strip()
                if text: lines.append(text)
        else:
            raw_text = content_ele.text
            for line in raw_text.split('\n'):
                line = line.strip()
                if line: lines.append(line)

        clean_lines = []
        for line in lines:
            if any(ad in line for ad in ["UUçœ‹ä¹¦", "uuks.org", "javascript", "è¯·æ”¶è—", "æœ¬ç«™"]):
                continue
            clean_lines.append(line)
        
        return '\n\n'.join(clean_lines)

    def generate_download_queue(self, chapters, novel_dir):
        """
        ã€æ–°å¢åŠŸèƒ½ã€‘ç”Ÿæˆæ™ºèƒ½ä¸‹è½½é˜Ÿåˆ—
        å¯¹æ¯”æœ¬åœ°æ–‡ä»¶ï¼Œå†³å®šå“ªäº›éœ€è¦ä¸‹è½½
        """
        print("[ç³»ç»Ÿ] æ­£åœ¨æ ¡éªŒæœ¬åœ°æ–‡ä»¶å®Œæ•´æ€§...")
        download_queue = []
        skipped_count = 0
        incomplete_count = 0

        # è¿™é‡Œä½¿ç”¨ start=1 ç¡®ä¿å’Œä¹‹å‰çš„å‘½åé€»è¾‘ä¸€è‡´
        for index, chapter in enumerate(chapters, start=1):
            safe_name = self.validate_filename(chapter['name'])
            # ä¿æŒ 0001_xxx.txt çš„å‘½åæ ¼å¼
            file_name = f"{str(index).zfill(4)}_{safe_name}.txt"
            file_path = os.path.join(novel_dir, file_name)

            should_download = False
            
            if not os.path.exists(file_path):
                # æƒ…å†µ1ï¼šæ–‡ä»¶ä¸å­˜åœ¨
                should_download = True
            else:
                # æƒ…å†µ2ï¼šæ–‡ä»¶å­˜åœ¨ï¼Œä½†ä½“ç§¯è¿‡å°ï¼ˆå°äº300å­—èŠ‚è§†ä¸ºä¸å®Œæ•´/æŠ¥é”™ï¼‰
                file_size = os.path.getsize(file_path)
                if file_size < 300:
                    should_download = True
                    incomplete_count += 1
                else:
                    skipped_count += 1
            
            if should_download:
                # å°† åºå·ã€æ–‡ä»¶åã€URL æ‰“åŒ…å­˜å…¥é˜Ÿåˆ—
                download_queue.append({
                    'index': index,
                    'name': safe_name,
                    'file_path': file_path,
                    'url': chapter['url']
                })

        return download_queue, skipped_count, incomplete_count

    def run(self):
        # 1. è§£æç›®å½•
        result = self.parse_catalog()
        if not result or not result[1]:
            return

        book_title, chapters = result
        novel_dir = os.path.join(self.base_save_path, book_title)
        
        if not os.path.exists(novel_dir):
            os.makedirs(novel_dir, exist_ok=True)

        # 2. ç”Ÿæˆä¸‹è½½é˜Ÿåˆ— (æ™ºèƒ½æ ¡éªŒ)
        queue, skipped, incomplete = self.generate_download_queue(chapters, novel_dir)

        total_chapters = len(chapters)
        total_tasks = len(queue)
        
        print("\n" + "="*50)
        print(f" ğŸ“š ä¹¦å: {book_title}")
        print(f" ğŸ“‘ æ€»ç« èŠ‚: {total_chapters}")
        print(f" âœ… å·²å®Œæˆ: {skipped}")
        print(f" âš ï¸ ä¸å®Œæ•´: {incomplete} (å°†é‡æ–°ä¸‹è½½)")
        print(f" â¬‡ï¸ å¾…ä¸‹è½½: {total_tasks}")
        print("="*50 + "\n")

        if total_tasks == 0:
            print("[æ­å–œ] æ‰€æœ‰ç« èŠ‚æ ¡éªŒå®Œæ•´ï¼Œæ— éœ€ä¸‹è½½ï¼")
            return

        print(f"[2/2] å¼€å§‹æ‰§è¡Œä¸‹è½½ä»»åŠ¡...")
        
        success_count = 0
        # éå†ä¸‹è½½é˜Ÿåˆ—
        for i, task in enumerate(queue, start=1):
            
            print(f"è¿›åº¦ ({i}/{total_tasks}) | æ­£åœ¨ä¸‹è½½: {task['name']}")
            
            content = None
            for retry in range(3):
                try:
                    content = self.parse_content(task['url'])
                    if content: break
                    time.sleep(1)
                except:
                    pass
            
            if content:
                try:
                    with open(task['file_path'], 'w', encoding='utf-8') as f:
                        f.write(content)
                    success_count += 1
                except Exception as e:
                    print(f"  [å†™å…¥é”™è¯¯] {e}")
            else:
                print(f"  [ä¸‹è½½å¤±è´¥] {task['url']}")
                with open(os.path.join(novel_dir, "error_log.txt"), "a", encoding="utf-8") as log:
                    log.write(f"{task['name']}: {task['url']}\n")
            
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(0.1, 0.3))

        print(f"\n[å®Œæˆ] ä»»åŠ¡ç»“æŸï¼æœ¬è½®æˆåŠŸä¸‹è½½ {success_count} ç« ã€‚")

if __name__ == "__main__":
    TARGET_URL = "https://www.uuks.org/b/73220/"
    BASE_PATH = r"E:\Programme\lncrawl\down" 

    print("=== å°è¯´ä¸‹è½½å™¨å¯åŠ¨ (æ™ºèƒ½å¢é‡æ›´æ–°ç‰ˆ) ===")
    downloader = NovelDownloader(TARGET_URL, BASE_PATH)
    downloader.run()
    input("æŒ‰å›è½¦é”®é€€å‡º...")
